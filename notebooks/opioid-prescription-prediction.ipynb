{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is configured to run on the VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note all dataframes should be named in respect to the below dataframe names\n",
    "# if inserted Pandas DataFrame, skip or delete this cell\n",
    "df_data_1 = pd.read_csv('../data/opioids.csv')\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if inserted Pandas DataFrame, skip or delete this cell\n",
    "df_data_2 = pd.read_csv('../data/overdoses.csv')\n",
    "df_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if inserted Pandas DataFrame, skip or delete this cell\n",
    "df_data_3 = pd.read_csv('../data/prescriber-info.csv')\n",
    "df_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start out by removing the ',' from our numbers in the Deaths and Population columns so that we can use them as integers\n",
    "df_data_2['Deaths'] = df_data_2['Deaths'].str.replace(',', '')\n",
    "df_data_2['Deaths'] = df_data_2['Deaths'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_2['Population'] = df_data_2['Population'].str.replace(',', '')\n",
    "df_data_2['Population'] = df_data_2['Population'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding an additional column where we see the deaths per capita per each state\n",
    "df_data_2['Deaths/Population'] = (df_data_2['Deaths']/df_data_2['Population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check to see that it worked!\n",
    "df_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pixiedust to visualize our initial exploration.\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "State",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "valueFields": "Deaths"
     }
    }
   },
   "outputs": [],
   "source": [
    "#How many opioid deaths by U.S. state?\n",
    "display(df_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It definitely looks like California has a great deal more deaths than any other state. \n",
    "#Let's remember, however, California is a huge state with a matching population. Because of this we need to take a look at the values of deaths per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "State",
      "orientation": "horizontal",
      "rowCount": "500",
      "valueFields": "Deaths/Population"
     }
    }
   },
   "outputs": [],
   "source": [
    "#What about deaths (% of population) by U.S. State?\n",
    "display(df_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that West Virginia, New Mexico, New Hampshire, Ohio, Kentucky and Delaware stand out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "googlemapapikey": " AIzaSyDi-BV6kv6RzFOB8-F8454uvk1tzYseFVg ",
      "handlerId": "mapView",
      "keyFields": "State",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rendererId": "google",
      "rowCount": "500",
      "title": "Deaths per Capita by Opioid Overdoses",
      "valueFields": "Deaths/Population"
     }
    }
   },
   "outputs": [],
   "source": [
    "#Let's check this out with a map. (I used google maps. To do this, create an API and enable JavaScript and GeoCoding. Then use your API key under 'Options'.)\n",
    "display(df_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's move onto exploring our other dataset.\n",
    "df_data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We seem to have a great deal of prescriptions as well as physicians' gender, state, speciality, whether they are an opioid prescriber or not and unique ID.\n",
    "df_data_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the states. Why are there more than 50 states?\n",
    "df_data_3.State.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare to df_data_2.\n",
    "df_data_2.Abbrev.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up states and make the dataset state list equal.\n",
    "#I checked the list of US state abbreviations and did not recognize PR, AE, ZZ, GU, AA or VI. After checking I learned that PR is Puerto Rico, GU is Guam and VI is Virgin Islands.\n",
    "#Though I identified 3 of the 6 unknowns, I'll remove all of them as dataset 2 does not have data regarding PR, GU or VI.\n",
    "df_data_3 = df_data_3[df_data_3.State != 'AE']\n",
    "df_data_3 = df_data_3[df_data_3.State != 'ZZ']\n",
    "df_data_3 = df_data_3[df_data_3.State != 'AA']\n",
    "df_data_3 = df_data_3[df_data_3.State != 'PR']\n",
    "df_data_3 = df_data_3[df_data_3.State != 'GU']\n",
    "df_data_3 = df_data_3[df_data_3.State != 'VI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure it worked!\n",
    "df_data_3.State.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out how many credentials there are.\n",
    "df_data_3.Credentials.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out the specialties.\n",
    "df_data_3.Specialty.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How much of the dataset is male vs female?\n",
    "df_data_3.groupby('Gender').size() / df_data_3.groupby('Gender').size().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many prescribers in our dataset prescribe opioid drugs vs do not?\n",
    "df_data_3.groupby('Opioid.Prescriber').size() / df_data_3.groupby('Opioid.Prescriber').size().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the opioid prescriber count vs non opioid prescriber count.\n",
    "#The dataset has a slightly higher number of opioid prescribers.\n",
    "pd.value_counts(df_data_3['Opioid.Prescriber']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Classifiers to Predict Opioid Prescribers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of Approaches: Kaggle and Indiana University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - \"Quick and Dirty\" approach from Kaggle (https://www.kaggle.com/jiashenliu/quick-and-dirty-attempt-on-voting-classifier)\n",
    " - \"Detecting Frequent Opioid Prescription\" (https://inclass.kaggle.com/apryor6/detecting-frequent-opioid-prescription)\n",
    " - Indiana University: \"Opiate prescription analysis using machine learning\" (http://cgi.soic.indiana.edu/~arunsank/AML_report.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the shape of our data frame so that we know how to set our classifiers up.\n",
    "print(df_data_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mark opioid vs non opiod drugs in df_data_3 with use of df_data_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioids = df_data_1 \n",
    "name=opioids['Drug Name']\n",
    "import re\n",
    "new_name=name.apply(lambda x:re.sub(\"\\ |-\",\".\",str(x)))\n",
    "columns=df_data_3.columns\n",
    "Abandoned_variables = set(columns).intersection(set(new_name))\n",
    "Kept_variable=[]\n",
    "for each in columns:\n",
    "    if each in Abandoned_variables:\n",
    "        pass\n",
    "    else:\n",
    "        Kept_variable.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at our new shape.\n",
    "df=df_data_3[Kept_variable]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's remove the credentials column so that we can use the speciality column instead. \n",
    "#We will also remove the NPI column in order to trim our features down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0, 3]], axis=1) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's now create our training and test data.\n",
    "train,test = train_test_split(df,test_size=0.2,random_state=42)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we convert our categorical columns.\n",
    "Categorical_columns=['Gender','State','Specialty']\n",
    "\n",
    "for col in Categorical_columns:\n",
    "    train[col]=pd.factorize(train[col], sort=True)[0]\n",
    "    test[col] =pd.factorize(test[col],sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set our features.\n",
    "features=train.iloc[:,1:242] #make sure we only use the columns that we want as our features\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import.\n",
    "import sklearn\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train our models. Let's use several classifiers so that we can check out which has the highest accuracy.\n",
    "#Added bagging classifier to check for overfitting (along with cross validation).\n",
    "#With 'Gender' included.\n",
    "features=train.iloc[:,0:242] #Make sure to remove Opioid.Prescriber (our target)!\n",
    "target = train['Opioid.Prescriber']\n",
    "Name=[]\n",
    "Accuracy=[]\n",
    "model1=LogisticRegression(random_state=22,C=0.000000001,solver='liblinear',max_iter=200)\n",
    "model2=GaussianNB()\n",
    "model3=RandomForestClassifier(n_estimators=200,random_state=22)\n",
    "model4=GradientBoostingClassifier(n_estimators=200)\n",
    "model5=KNeighborsClassifier()\n",
    "model6=DecisionTreeClassifier()\n",
    "model7=LinearDiscriminantAnalysis()\n",
    "model8=BaggingClassifier()\n",
    "Ensembled_model=VotingClassifier(estimators=[('lr', model1), ('gn', model2), ('rf', model3),('gb',model4),('kn',model5),('dt',model6),('lda',model7), ('bc',model8)], voting='hard')\n",
    "for model, label in zip([model1, model2, model3, model4,model5,model6,model7,model8,Ensembled_model], ['Logistic Regression','Naive Bayes','Random Forest', 'Gradient Boosting','KNN','Decision Tree','LDA', 'Bagging Classifier', 'Ensemble']):\n",
    "    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n",
    "    Accuracy.append(scores.mean())\n",
    "    Name.append(model.__class__.__name__)\n",
    "    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender not included.\n",
    "features=train.iloc[:,1:242] #Make sure to remove Opioid.Prescriber (our target)!\n",
    "target = train['Opioid.Prescriber']\n",
    "Name=[]\n",
    "Accuracy=[]\n",
    "model1=LogisticRegression(random_state=22,C=0.000000001,solver='liblinear',max_iter=200)\n",
    "model2=GaussianNB()\n",
    "model3=RandomForestClassifier(n_estimators=200,random_state=22)\n",
    "model4=GradientBoostingClassifier(n_estimators=200)\n",
    "model5=KNeighborsClassifier()\n",
    "model6=DecisionTreeClassifier()\n",
    "model7=LinearDiscriminantAnalysis()\n",
    "model8=BaggingClassifier()\n",
    "Ensembled_model=VotingClassifier(estimators=[('lr', model1), ('gn', model2), ('rf', model3),('gb',model4),('kn',model5),('dt',model6),('lda',model7), ('bc',model8)], voting='hard')\n",
    "for model, label in zip([model1, model2, model3, model4,model5,model6,model7,model8,Ensembled_model], ['Logistic Regression','Naive Bayes','Random Forest', 'Gradient Boosting','KNN','Decision Tree','LDA', 'Bagging Classifier', 'Ensemble']):\n",
    "    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n",
    "    Accuracy.append(scores.mean())\n",
    "    Name.append(model.__class__.__name__)\n",
    "    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like our highest accuracy score is 83.5% with Random Forest, followed by 83.3% with the Ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall, it seems our models are less accurate when 'Gender' is included, with the exception of our Ensemble which gets a fairly high accuracy at 83.4% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check out our best models from our run without 'Gender'.\n",
    "Name_2=[]\n",
    "Accuracy_2=[]\n",
    "Ensembled_model_3=VotingClassifier(estimators=[('rf', model3),('em',Ensembled_model)], voting='hard')\n",
    "for model, label in zip([model3, model4,Ensembled_model_3, model8], ['Random Forest', 'Gradient Boosting', 'Ensemble', 'Bagging Classifier']):\n",
    "    scores = cross_val_score(model, features, target, cv=5, scoring='accuracy')\n",
    "    Accuracy_2.append(scores.mean())\n",
    "    Name_2.append(model.__class__.__name__)\n",
    "    print(\"Accuracy: %f of model %s\" % (scores.mean(),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "classifers=[model3,model4,model8]\n",
    "out_sample_accuracy=[]\n",
    "Name_2=[]\n",
    "for each in classifers:\n",
    "    fit=each.fit(features,target)\n",
    "    pred=fit.predict(test.iloc[:,1:242])\n",
    "    accuracy=accuracy_score(test['Opioid.Prescriber'],pred)\n",
    "    Name_2.append(each.__class__.__name__)\n",
    "    out_sample_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_actu = test['Opioid.Prescriber']\n",
    "confusion_matrix(y_actu, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall Curve\n",
    "sklearn.metrics.precision_recall_curve(y_actu, pred, pos_label=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall Score\n",
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_actu, pred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall Curve Plot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_actu, pred)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After running various classifiers, we find that Random Forest, Gradient Boosting and our Ensemble models had the best performance comparatively. This means that if we were to build a larger project, we could focus on these particular classifiers, building upon them to help predict opioid prescribers (given more years of data). \n",
    "\n",
    "The precision-recall curve can help us determine if we were successful enough. For the unfamiliar precision-recall scores represent a balance between high recall and high precision relating to a low false positive rate and a low false negative rate respectively. When evaluating, you have four outcomes: true positive, true negative, false positive and false negative. Depending on the project, you would aim for different balances, but ideally you want everything to be as accurate, or true, as possible. In the graph above, the y axis is the precision and the x axis is recall. If the graph went straight across the middle, that would be a random-like output. Below it would be poor performance and above it would be a more accurate and better quality performance. If it were 100 it would be a perfect classifier. Because the classifier above is at 0.84 we can feel confident that our precision was good. Now I challenge you to go improve it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
